{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import yfinance as yf\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SPACE = 28\n",
    "ACTION_SPACE = 3\n",
    "\n",
    "ACTION_LOW = -1\n",
    "ACTION_HIGH = 1\n",
    "\n",
    "GAMMA = 0.9995\n",
    "TAU = 1e-3\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 0.9\n",
    "\n",
    "MEMORY_LEN = 10000\n",
    "MEMORY_THRESH = 500\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "LR_DQN = 5e-4\n",
    "\n",
    "LEARN_AFTER = MEMORY_THRESH\n",
    "LEARN_EVERY = 5\n",
    "UPDATE_EVERY = 9\n",
    "\n",
    "COST = 3e-4\n",
    "CAPITAL = 100000\n",
    "NEG_MUL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Py torch\n",
    "\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "EPOCHS = 100\n",
    "BACTH_SIZE = 64\n",
    "\n",
    "# Define the learning rate\n",
    "LR = 5e-3\n",
    "\n",
    "# Define the discount factor\n",
    "GAMMA = 0.99\n",
    "\n",
    "# Memory capacity\n",
    "MEM_CAP = 10000\n",
    "MEM_MIN = 500 # learn after having this amount of memory\n",
    "\n",
    "ACTIONS = [-1, 0, 1]\n",
    "LEN_ACTIONS = len(ACTIONS)\n",
    "INPUT_DIM = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_DuellingDQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN_DuellingDQN, self).__init__()\n",
    "        self.input_dim = INPUT_DIM\n",
    "        self.output_dim = LEN_ACTIONS\n",
    "\n",
    "        self.l1 = nn.Linear(self.input_dim, 500)\n",
    "        self.l2 = nn.Linear(500, 500)\n",
    "        self.l3 = nn.Linear(500, 300)\n",
    "        self.l4 = nn.Linear(300, 200)\n",
    "        self.l5 = nn.Linear(200, 10)\n",
    "\n",
    "        self.ls = nn.Linear(10, 1)\n",
    "        self.lp = nn.Linear(10, self.output_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.relu(self.l1(state))\n",
    "        x = self.relu(self.l2(x))\n",
    "        x = self.relu(self.l3(x))\n",
    "        x = self.relu(self.l4(x))\n",
    "        x = self.relu(self.l5(x))\n",
    "        xs = self.relu(self.ls(x))\n",
    "        xp = self.relu(self.lp(x))\n",
    "\n",
    "        x = xs + xp - xp.mean()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortMemory:\n",
    "    def __init__(self, capacity=MEM_CAP):\n",
    "        self.memory = deque(maxlen=capacity) # queue to remove older mem cells\n",
    "\n",
    "    def store(self, output : tuple): # contains everything from env output\n",
    "        self.memory.append(output)\n",
    "        # state, new_state, action, reward, done\n",
    "\n",
    "    def sample(self, n):\n",
    "        output = random.sample(self.memory, n)\n",
    "        return output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.nnql = NN_DuellingDQN().to(DEVICE) # local\n",
    "        self.nnqt = NN_DuellingDQN().to(DEVICE) # target\n",
    "        self.nnqt.load_state_dict(self.nnql.state_dict())\n",
    "        self.nnqt.eval()\n",
    "\n",
    "        self.memory = ShortMemory()\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optim = torch.optim.Adam(self.nnql.parameters(), lr=LR)\n",
    "\n",
    "        self.epsilon = 1.0\n",
    "        self.step = 0\n",
    "\n",
    "    def inference_step(self, state): # forward pass\n",
    "        self.nnql.eval() # don't update nn\n",
    "        with torch.no_grad(): # don't update gradients\n",
    "            actions = self.nnql(state)\n",
    "        self.nnql.train() # THEN update => forward\n",
    "        return actions\n",
    "\n",
    "    def epsilon_greedy_pol(self, state, test=False) -> int:\n",
    "        if not test:\n",
    "            self.step += 1\n",
    "        state = torch.from_numpy(state).float().to(DEVICE).view(1, -1)\n",
    "\n",
    "        actions = self.inference_step(state)\n",
    "        if random.random() > self.epsilon or test == True:\n",
    "            act = np.argmax(actions.cpu().data.numpy())\n",
    "        else:\n",
    "            act = random.choice(np.arange(LEN_ACTIONS))\n",
    "        return ACTIONS[int(act)]\n",
    "\n",
    "    def calc_bellman(self, states, next_states, actions, rewards, dones) -> tuple:\n",
    "        next_state_values = self.nnqt(next_states).max(1)[0].unsqueeze(1)\n",
    "        y = rewards + (1-dones) * GAMMA * next_state_values\n",
    "        state_values = self.nnql(states).gather(1, actions.type(torch.int64).add(1))\n",
    "        return y, state_values\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) <= MEM_MIN:\n",
    "            return\n",
    "\n",
    "        if self.step % LEARN_EVERY != 0:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(BACTH_SIZE)\n",
    "\n",
    "        states = np.vstack([t[0] for t in batch])\n",
    "        states = torch.from_numpy(states).float().to(DEVICE)\n",
    "\n",
    "        next_states = np.vstack([t[1] for t in batch])\n",
    "        next_states = torch.from_numpy(next_states).float().to(DEVICE)\n",
    "\n",
    "        actions = np.vstack([t[2] for t in batch])\n",
    "        actions = torch.from_numpy(actions).float().to(DEVICE)\n",
    "\n",
    "        rewards = np.vstack([t[3] for t in batch])\n",
    "        rewards = torch.from_numpy(rewards).float().to(DEVICE)\n",
    "\n",
    "        dones = np.vstack([t[4] for t in batch]).astype(np.uint8)\n",
    "        dones = torch.from_numpy(dones).float().to(DEVICE)\n",
    "\n",
    "        # Bellman :\n",
    "        y, state_values = self.calc_bellman(states, next_states, actions, rewards, dones)\n",
    "\n",
    "        loss = self.criterion(y, state_values)\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "        if self.step % UPDATE_EVERY == 0:\n",
    "            self.update_param()\n",
    "\n",
    "    def update_param(self):\n",
    "        for target_param, local_param in zip(self.nnqt.parameters(), self.nnql.parameters()):\n",
    "            target_param.data.copy_(LR * local_param.data + (1.0 - LR) * target_param.data)\n",
    "\n",
    "    def train(self, env, epochs : int) -> list:\n",
    "        scores = []\n",
    "        eps = EPS_START\n",
    "        self.step = 0\n",
    "        for e in range(epochs):\n",
    "            print(\"epochs nb:\", e, \"epsilon:\", self.epsilon)\n",
    "            episode_score = 0\n",
    "            episode_score2 = 0\n",
    "            test_score = 0\n",
    "            test_score2 = 0\n",
    "            score = 0\n",
    "            state = env.reset()\n",
    "            state = state.reshape(-1, INPUT_DIM)\n",
    "            while True:\n",
    "                action = self.epsilon_greedy_pol(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = next_state.reshape(-1, INPUT_DIM)\n",
    "\n",
    "                self.memory.store((state, next_state, action, reward, done))\n",
    "                self.learn()\n",
    "\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            self.epsilon = max(self.epsilon * 0.9, 0.1)\n",
    "            \n",
    "            episode_score += score\n",
    "            episode_score2 += (env.store['running_capital'][-1] - env.store['running_capital'][0])\n",
    "            scores.append(score)\n",
    "            \n",
    "            state = test_env.reset()\n",
    "            done = False\n",
    "            score_te = 0\n",
    "            scores_te = [score_te]\n",
    "            while True:\n",
    "                action = self.epsilon_greedy_pol(state)\n",
    "                next_state, reward, done, _ = test_env.step(action)\n",
    "                next_state = next_state.reshape(-1, STATE_SPACE)\n",
    "                state= next_state\n",
    "                score_te += reward\n",
    "                scores_te.append(score_te)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            test_score += score_te\n",
    "            test_score2 += (test_env.store['running_capital'][-1] - test_env.store['running_capital'][0])\n",
    "\n",
    "            print(f\"Episode: {e}, Train Score: {episode_score:.5f}, Validation Score: {test_score:.5f}\")\n",
    "            print(f\"Episode: {e}, Train Value: ${episode_score2:.5f}, Validation Value: ${test_score2:.5f}\", \"\\n\")\n",
    "        \n",
    "        df = pd.DataFrame(list(zip(env.store['action_store'], env.store['running_capital'])),\n",
    "                                   columns =['Actions', 'Capital', ])\n",
    "        df.to_csv(\"./out.csv\", index=False)\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGetter:\n",
    "  \"\"\"\n",
    "  The class for getting data for assets.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, asset=\"BTC-USD\", start_date=None, end_date=None, freq=\"1d\",\n",
    "               timeframes=[1, 2, 5, 10, 20, 40]):\n",
    "    self.asset = asset\n",
    "    self.sd = start_date\n",
    "    self.ed = end_date\n",
    "    self.freq = freq\n",
    "\n",
    "    self.timeframes = timeframes\n",
    "    self.getData()\n",
    "\n",
    "    self.scaler = StandardScaler()\n",
    "    self.scaler.fit(self.data[:, 1:])\n",
    "\n",
    "\n",
    "  def getData(self):\n",
    "\n",
    "    asset = self.asset\n",
    "    if self.sd is not None and self.ed is not None:\n",
    "      df =  yf.download([asset], start=self.sd, end=self.ed, interval=self.freq)\n",
    "      df_spy = yf.download([\"BTC-USD\"], start=self.sd, end=self.ed, interval=self.freq)\n",
    "    elif self.sd is None and self.ed is not None:\n",
    "      df =  yf.download([asset], end=self.ed, interval=self.freq)\n",
    "      df_spy = yf.download([\"BTC-USD\"], end=self.ed, interval=self.freq)\n",
    "    elif self.sd is not None and self.ed is None:\n",
    "      df =  yf.download([asset], start=self.sd, interval=self.freq)\n",
    "      df_spy = yf.download([\"BTC-USD\"], start=self.sd, interval=self.freq)\n",
    "    else:\n",
    "      df = yf.download([asset], period=\"max\", interval=self.freq)\n",
    "      df_spy = yf.download([\"BTC-USD\"], interval=self.freq)\n",
    "\n",
    "    # Reward - Not included in Observation Space.\n",
    "    df[\"rf\"] = df[\"Adj Close\"].pct_change().shift(-1)\n",
    "\n",
    "    # Returns and Trading Volume Changes\n",
    "    for i in self.timeframes:\n",
    "      df_spy[f\"spy_ret-{i}\"] = df_spy[\"Adj Close\"].pct_change(i)\n",
    "      df_spy[f\"spy_v-{i}\"] = df_spy[\"Volume\"].pct_change(i)\n",
    "\n",
    "      df[f\"r-{i}\"] = df[\"Adj Close\"].pct_change(i)\n",
    "      df[f\"v-{i}\"] = df[\"Volume\"].pct_change(i)\n",
    "\n",
    "    # Volatility\n",
    "    for i in [5, 10, 20, 40]:\n",
    "      df[f'sig-{i}'] = np.log(1 + df[\"r-1\"]).rolling(i).std()\n",
    "\n",
    "    # Moving Average Convergence Divergence (MACD)\n",
    "    df[\"macd_lmw\"] = df[\"r-1\"].ewm(span=26, adjust=False).mean()\n",
    "    df[\"macd_smw\"] = df[\"r-1\"].ewm(span=12, adjust=False).mean()\n",
    "    df[\"macd_bl\"] = df[\"r-1\"].ewm(span=9, adjust=False).mean()\n",
    "    df[\"macd\"] = df[\"macd_smw\"] - df[\"macd_lmw\"]\n",
    "\n",
    "    # Relative Strength Indicator (RSI)\n",
    "    rsi_lb = 5\n",
    "    pos_gain = df[\"r-1\"].where(df[\"r-1\"] > 0, 0).ewm(rsi_lb).mean()\n",
    "    neg_gain = df[\"r-1\"].where(df[\"r-1\"] < 0, 0).ewm(rsi_lb).mean()\n",
    "    rs = np.abs(pos_gain/neg_gain)\n",
    "    df[\"rsi\"] = 100 * rs/(1 + rs)\n",
    "\n",
    "    # Bollinger Bands\n",
    "    bollinger_lback = 10\n",
    "    df[\"bollinger\"] = df[\"r-1\"].ewm(bollinger_lback).mean()\n",
    "    df[\"low_bollinger\"] = df[\"bollinger\"] - 2 * df[\"r-1\"].rolling(bollinger_lback).std()\n",
    "    df[\"high_bollinger\"] = df[\"bollinger\"] + 2 * df[\"r-1\"].rolling(bollinger_lback).std()\n",
    "\n",
    "    # Filtering\n",
    "    for c in df.columns:\n",
    "      df[c].interpolate('linear', limit_direction='both', inplace=True)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df.to_csv('./prices.csv')\n",
    "\n",
    "    self.frame = df\n",
    "    self.data = np.array(df.iloc[:, 6:])\n",
    "    return\n",
    "\n",
    "\n",
    "  def scaleData(self):\n",
    "    self.scaled_data = self.scaler.fit_transform(self.data[:, 1:])\n",
    "    return\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "\n",
    "  def __getitem__(self, idx, col_idx=None):\n",
    "    if col_idx is None:\n",
    "      return self.data[idx]\n",
    "    elif col_idx < len(list(self.data.columns)):\n",
    "      return self.data[idx][col_idx]\n",
    "    else:\n",
    "      raise IndexError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAssetTradingEnvironment:\n",
    "  \"\"\"\n",
    "  Trading Environment for trading a single asset.\n",
    "  The Agent interacts with the environment class through the step() function.\n",
    "  Action Space: {-1: Sell, 0: Do Nothing, 1: Buy}\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, asset_data,\n",
    "               initial_money=CAPITAL, trans_cost=COST, store_flag=1, asset_ph=0, \n",
    "               capital_frac=0.2, running_thresh=0.1, cap_thresh=0.3):\n",
    "\n",
    "    self.past_holding = asset_ph\n",
    "    self.capital_frac = capital_frac # Fraction of capital to invest each time.\n",
    "    self.cap_thresh = cap_thresh\n",
    "    self.running_thresh = running_thresh\n",
    "    self.trans_cost = trans_cost\n",
    "\n",
    "    self.asset_data = asset_data\n",
    "    self.terminal_idx = len(self.asset_data) - 1\n",
    "    self.scaler = self.asset_data.scaler    \n",
    "\n",
    "    self.initial_cap = initial_money\n",
    "\n",
    "    self.capital = self.initial_cap\n",
    "    self.running_capital = self.capital\n",
    "    self.asset_inv = self.past_holding\n",
    "\n",
    "    self.pointer = 0\n",
    "    self.next_return, self.current_state = 0, None\n",
    "    self.prev_act = 0\n",
    "    self.current_act = 0\n",
    "    self.current_reward = 0\n",
    "    self.current_price = self.asset_data.frame.iloc[self.pointer, :]['Adj Close']\n",
    "    self.done = False\n",
    "\n",
    "    self.store_flag = store_flag\n",
    "    if self.store_flag == 1:\n",
    "      self.store = {\"action_store\": [],\n",
    "                    \"reward_store\": [],\n",
    "                    \"running_capital\": [],\n",
    "                    \"port_ret\": []}\n",
    "\n",
    "  def reset(self):\n",
    "    self.capital = self.initial_cap\n",
    "    self.running_capital = self.capital\n",
    "    self.asset_inv = self.past_holding\n",
    "\n",
    "    self.pointer = 0\n",
    "    self.next_return, self.current_state = self.get_state(self.pointer)\n",
    "    self.prev_act = 0\n",
    "    self.current_act = 0\n",
    "    self.current_reward = 0\n",
    "    self.current_price = self.asset_data.frame.iloc[self.pointer, :]['Adj Close']\n",
    "    self.done = False\n",
    "    \n",
    "    if self.store_flag == 1:\n",
    "      self.store = {\"action_store\": [],\n",
    "                    \"reward_store\": [],\n",
    "                    \"running_capital\": [],\n",
    "                    \"port_ret\": []}\n",
    "\n",
    "    return self.current_state\n",
    "\n",
    "  def step(self, action):\n",
    "    self.current_act = action\n",
    "    self.current_price = self.asset_data.frame.iloc[self.pointer, :]['Adj Close']\n",
    "    self.current_reward = self.calculate_reward()\n",
    "    self.prev_act = self.current_act\n",
    "    self.pointer += 1\n",
    "    self.next_return, self.current_state = self.get_state(self.pointer)\n",
    "    self.done = self.check_terminal()\n",
    "\n",
    "    if self.done:\n",
    "      reward_offset = 0\n",
    "      ret = (self.store['running_capital'][-1]/self.store['running_capital'][-0]) - 1\n",
    "      if self.pointer < self.terminal_idx:\n",
    "        reward_offset += -1 * max(0.5, 1 - self.pointer/self.terminal_idx)\n",
    "      if self.store_flag:\n",
    "        reward_offset += 10 * ret\n",
    "      self.current_reward += reward_offset\n",
    "\n",
    "    if self.store_flag:\n",
    "      self.store[\"action_store\"].append(self.current_act)\n",
    "      self.store[\"reward_store\"].append(self.current_reward)\n",
    "      self.store[\"running_capital\"].append(self.capital)\n",
    "      info = self.store\n",
    "    else:\n",
    "      info = None\n",
    "    \n",
    "    return self.current_state, self.current_reward, self.done, info\n",
    "\n",
    "\n",
    "  def calculate_reward(self):\n",
    "    investment = self.running_capital * self.capital_frac\n",
    "    reward_offset = 0\n",
    "\n",
    "    # Buy Action\n",
    "    if self.current_act == 1: \n",
    "      if self.running_capital > self.initial_cap * self.running_thresh:\n",
    "        self.running_capital -= investment\n",
    "        asset_units = investment/self.current_price\n",
    "        self.asset_inv += asset_units\n",
    "        self.current_price *= (1 - self.trans_cost)\n",
    "\n",
    "    # Sell Action\n",
    "    elif self.current_act == -1:\n",
    "      if self.asset_inv > 0:\n",
    "        self.running_capital += self.asset_inv * self.current_price * (1 - self.trans_cost)\n",
    "        self.asset_inv = 0\n",
    "\n",
    "    # Do Nothing\n",
    "    elif self.current_act == 0:\n",
    "      if self.prev_act == 0:\n",
    "        reward_offset += -0.1\n",
    "      pass\n",
    "    \n",
    "    # Reward to give\n",
    "    prev_cap = self.capital\n",
    "    self.capital = self.running_capital + (self.asset_inv) * self.current_price\n",
    "    reward = 100*(self.next_return) * self.current_act - np.abs(self.current_act - self.prev_act) * self.trans_cost\n",
    "    if self.store_flag==1:\n",
    "      self.store['port_ret'].append((self.capital - prev_cap)/prev_cap)\n",
    "    \n",
    "    if reward < 0:\n",
    "      reward *= NEG_MUL  # To make the Agent more risk averse towards negative returns.\n",
    "    reward += reward_offset\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "  def check_terminal(self):\n",
    "    if self.pointer == self.terminal_idx:\n",
    "      return True\n",
    "    elif self.capital <= self.initial_cap * self.cap_thresh:\n",
    "      return True\n",
    "    else:\n",
    "      return False\n",
    "\n",
    "\n",
    "  def get_state(self, idx):\n",
    "    state = self.asset_data[idx][1:]\n",
    "    state = self.scaler.transform(state.reshape(1, -1))\n",
    "\n",
    "    state = np.concatenate([state, [[self.capital/self.initial_cap,\n",
    "                                     self.running_capital/self.capital,\n",
    "                                     self.asset_inv * self.current_price/self.initial_cap,\n",
    "                                     self.prev_act]]], axis=-1)\n",
    "\n",
    "    next_ret = self.asset_data[idx][0]\n",
    "    return next_ret, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", [\"States\", \"Actions\", \"Rewards\", \"NextStates\", \"Dones\"])\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "  \"\"\"\n",
    "  Implementation of Agent memory\n",
    "  \"\"\"\n",
    "  def __init__(self, capacity=MEMORY_LEN):\n",
    "    self.memory = deque(maxlen=capacity)\n",
    "\n",
    "  def store(self, t):\n",
    "    self.memory.append(t)\n",
    "\n",
    "  def sample(self, n):\n",
    "    a = random.sample(self.memory, n)\n",
    "    return a\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "epochs nb: 0 epsilon: 1.0\n",
      "Episode: 0, Train Score: -1436.44821, Validation Score: -185.41800\n",
      "Episode: 0, Train Value: $-21278.42475, Validation Value: $-371.42231 \n",
      "\n",
      "epochs nb: 1 epsilon: 0.9\n",
      "Episode: 1, Train Score: -1454.26417, Validation Score: -259.37298\n",
      "Episode: 1, Train Value: $19721.02223, Validation Value: $-15278.72783 \n",
      "\n",
      "epochs nb: 2 epsilon: 0.81\n",
      "Episode: 2, Train Score: -1454.86808, Validation Score: -259.86392\n",
      "Episode: 2, Train Value: $50409.88786, Validation Value: $-10102.79240 \n",
      "\n",
      "epochs nb: 3 epsilon: 0.7290000000000001\n",
      "Episode: 3, Train Score: -769.45864, Validation Score: -154.87437\n",
      "Episode: 3, Train Value: $216161.56147, Validation Value: $8148.82753 \n",
      "\n",
      "epochs nb: 4 epsilon: 0.6561000000000001\n",
      "Episode: 4, Train Score: -682.43081, Validation Score: -166.58054\n",
      "Episode: 4, Train Value: $559326.71586, Validation Value: $12288.59351 \n",
      "\n",
      "epochs nb: 5 epsilon: 0.5904900000000002\n",
      "Episode: 5, Train Score: -1050.35900, Validation Score: -179.07708\n",
      "Episode: 5, Train Value: $209302.37625, Validation Value: $6122.09109 \n",
      "\n",
      "epochs nb: 6 epsilon: 0.5314410000000002\n",
      "Episode: 6, Train Score: -473.44339, Validation Score: -256.80173\n",
      "Episode: 6, Train Value: $380635.66081, Validation Value: $-12769.39822 \n",
      "\n",
      "epochs nb: 7 epsilon: 0.47829690000000014\n",
      "Episode: 7, Train Score: 174.71340, Validation Score: -116.17659\n",
      "Episode: 7, Train Value: $971916.61131, Validation Value: $-724.51427 \n",
      "\n",
      "epochs nb: 8 epsilon: 0.43046721000000016\n",
      "Episode: 8, Train Score: 30.62430, Validation Score: -237.85153\n",
      "Episode: 8, Train Value: $1113736.88654, Validation Value: $-582.71297 \n",
      "\n",
      "epochs nb: 9 epsilon: 0.38742048900000015\n",
      "Episode: 9, Train Score: 565.53584, Validation Score: -231.87828\n",
      "Episode: 9, Train Value: $2244022.16974, Validation Value: $4062.89606 \n",
      "\n",
      "epochs nb: 10 epsilon: 0.34867844010000015\n",
      "Episode: 10, Train Score: 1077.56026, Validation Score: -376.33344\n",
      "Episode: 10, Train Value: $4740149.21891, Validation Value: $-22652.24890 \n",
      "\n",
      "epochs nb: 11 epsilon: 0.31381059609000017\n",
      "Episode: 11, Train Score: 1316.05060, Validation Score: -147.84488\n",
      "Episode: 11, Train Value: $5387379.95064, Validation Value: $-10294.95326 \n",
      "\n",
      "epochs nb: 12 epsilon: 0.28242953648100017\n",
      "Episode: 12, Train Score: 1192.70301, Validation Score: -247.80842\n",
      "Episode: 12, Train Value: $2508107.74702, Validation Value: $1007.24933 \n",
      "\n",
      "epochs nb: 13 epsilon: 0.25418658283290013\n",
      "Episode: 13, Train Score: 1693.47946, Validation Score: -196.87785\n",
      "Episode: 13, Train Value: $8159070.69358, Validation Value: $23211.03531 \n",
      "\n",
      "epochs nb: 14 epsilon: 0.22876792454961012\n",
      "Episode: 14, Train Score: 2041.16625, Validation Score: -214.73639\n",
      "Episode: 14, Train Value: $8443875.49230, Validation Value: $10373.79272 \n",
      "\n",
      "epochs nb: 15 epsilon: 0.2058911320946491\n",
      "Episode: 15, Train Score: 1910.24973, Validation Score: -314.98390\n",
      "Episode: 15, Train Value: $7631087.43024, Validation Value: $5627.32555 \n",
      "\n",
      "epochs nb: 16 epsilon: 0.1853020188851842\n",
      "Episode: 16, Train Score: 1713.19973, Validation Score: -167.50174\n",
      "Episode: 16, Train Value: $4265185.78320, Validation Value: $4093.78465 \n",
      "\n",
      "epochs nb: 17 epsilon: 0.16677181699666577\n",
      "Episode: 17, Train Score: 1926.37441, Validation Score: -174.17386\n",
      "Episode: 17, Train Value: $5505734.63829, Validation Value: $9394.68145 \n",
      "\n",
      "epochs nb: 18 epsilon: 0.1500946352969992\n",
      "Episode: 18, Train Score: 2937.97918, Validation Score: -266.93597\n",
      "Episode: 18, Train Value: $11534673.32583, Validation Value: $4257.50538 \n",
      "\n",
      "epochs nb: 19 epsilon: 0.13508517176729928\n",
      "Episode: 19, Train Score: 6896.80870, Validation Score: -296.53310\n",
      "Episode: 19, Train Value: $53535051.80655, Validation Value: $60.15959 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-1436.4482063793046,\n",
       " -1454.2641714588653,\n",
       " -1454.8680839411863,\n",
       " -769.4586351805416,\n",
       " -682.4308135192498,\n",
       " -1050.3590025713015,\n",
       " -473.4433876659137,\n",
       " 174.71339841820748,\n",
       " 30.624297624911932,\n",
       " 565.5358360452492,\n",
       " 1077.5602590890048,\n",
       " 1316.0506031094674,\n",
       " 1192.7030105899935,\n",
       " 1693.4794562002412,\n",
       " 2041.1662491721277,\n",
       " 1910.2497306030039,\n",
       " 1713.1997323957885,\n",
       " 1926.3744106269207,\n",
       " 2937.979176165729,\n",
       " 6896.808702257504]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environment and Agent Initiation\n",
    "\n",
    "## Cryptocurrency Tickers\n",
    "asset_code = \"ETH-USD\"\n",
    "\n",
    "## Training and Testing Environments\n",
    "assets = DataGetter(asset_code, start_date=\"2015-01-01\", end_date=\"2021-10-01\")\n",
    "test_assets = DataGetter(asset_code, start_date=\"2021-10-01\", end_date=\"2022-05-01\", freq=\"1d\")\n",
    "env = SingleAssetTradingEnvironment(assets)\n",
    "test_env = SingleAssetTradingEnvironment(test_assets)\n",
    "\n",
    "## Agent\n",
    "memory = ReplayMemory()\n",
    "agent = Agent()\n",
    "\n",
    "# Main training loop\n",
    "N_EPISODES = 20 # No of episodes/epochs\n",
    "\n",
    "agent.train(env, int(N_EPISODES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
