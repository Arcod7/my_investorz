{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import yfinance as yf\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SPACE = 28\n",
    "ACTION_SPACE = 3\n",
    "\n",
    "ACTION_LOW = -1\n",
    "ACTION_HIGH = 1\n",
    "\n",
    "GAMMA = 0.9995\n",
    "TAU = 1e-3\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 0.9\n",
    "\n",
    "MEMORY_LEN = 10000\n",
    "MEMORY_THRESH = 500\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "LR_DQN = 5e-4\n",
    "\n",
    "LEARN_AFTER = MEMORY_THRESH\n",
    "LEARN_EVERY = 5\n",
    "UPDATE_EVERY = 9\n",
    "\n",
    "COST = 3e-4\n",
    "CAPITAL = 100000\n",
    "NEG_MUL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Py torch\n",
    "\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "EPOCHS = 100\n",
    "BACTH_SIZE = 64\n",
    "\n",
    "# Define the learning rate\n",
    "LR = 5e-3\n",
    "\n",
    "# Define the discount factor\n",
    "GAMMA = 0.99\n",
    "\n",
    "# Memory capacity\n",
    "MEM_CAP = 10000\n",
    "MEM_MIN = 500 # learn after having this amount of memory\n",
    "\n",
    "ACTIONS = [-1, 0, 1]\n",
    "LEN_ACTIONS = len(ACTIONS)\n",
    "INPUT_DIM = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_DuellingDQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN_DuellingDQN, self).__init__()\n",
    "        self.input_dim = INPUT_DIM\n",
    "        self.output_dim = LEN_ACTIONS\n",
    "\n",
    "        self.l1 = nn.Linear(self.input_dim, 500)\n",
    "        self.l2 = nn.Linear(500, 500)\n",
    "        self.l3 = nn.Linear(500, 300)\n",
    "        self.l4 = nn.Linear(300, 200)\n",
    "        self.l5 = nn.Linear(200 10)\n",
    "\n",
    "        self.ls = nn.Linear(10 1)\n",
    "        self.lp = nn.Linear(10, self.output_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.relu(self.l1(state))\n",
    "        x = self.relu(self.l2(x))\n",
    "        x = self.relu(self.l3(x))\n",
    "        x = self.relu(self.l4(x))\n",
    "        x = self.relu(self.l5(x))\n",
    "        xs = self.relu(self.ls(x))\n",
    "        xp = self.relu(self.lp(x))\n",
    "\n",
    "        x = xs + xp - xp.mean()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortMemory:\n",
    "    def __init__(self, capacity=MEM_CAP):\n",
    "        self.memory = deque(maxlen=capacity) # queue to remove older mem cells\n",
    "\n",
    "    def store(self, output : tuple): # contains everything from env output\n",
    "        self.memory.append(output)\n",
    "        # state, new_state, action, reward, done\n",
    "\n",
    "    def sample(self, n):\n",
    "        output = random.sample(self.memory, n)\n",
    "        return output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.nnql = NN_DuellingDQN().to(DEVICE) # local\n",
    "        self.nnqt = NN_DuellingDQN().to(DEVICE) # target\n",
    "        self.nnqt.load_state_dict(self.nnql.state_dict())\n",
    "        self.nnqt.eval()\n",
    "\n",
    "        self.memory = ShortMemory()\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optim = torch.optim.Adam(self.nnql.parameters(), lr=LR)\n",
    "\n",
    "        self.epsilon = 1.0\n",
    "        self.step = 0\n",
    "\n",
    "    def inference_step(self, state): # forward pass\n",
    "        self.nnql.eval() # don't update nn\n",
    "        with torch.no_grad(): # don't update gradients\n",
    "            actions = self.nnql(state)\n",
    "        self.nnql.train() # THEN update => forward\n",
    "        return actions\n",
    "\n",
    "    def epsilon_greedy_pol(self, state, test=False) -> int:\n",
    "        if not test:\n",
    "            self.step += 1\n",
    "        state = torch.from_numpy(state).float().to(DEVICE).view(1, -1)\n",
    "\n",
    "        actions = self.inference_step(state)\n",
    "        if random.random() > self.epsilon or test == True:\n",
    "            act = np.argmax(actions.cpu().data.numpy())\n",
    "        else:\n",
    "            act = random.choice(np.arange(LEN_ACTIONS))\n",
    "        return ACTIONS[int(act)]\n",
    "\n",
    "    def calc_bellman(self, states, next_states, actions, rewards, dones) -> tuple:\n",
    "        next_state_values = self.nnqt(next_states).max(1)[0].unsqueeze(1)\n",
    "        y = rewards + (1-dones) * GAMMA * next_state_values\n",
    "        state_values = self.nnql(states).gather(1, actions.type(torch.int64).add(1))\n",
    "        return y, state_values\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) <= MEM_MIN:\n",
    "            return\n",
    "\n",
    "        if self.step % LEARN_EVERY != 0:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(BACTH_SIZE)\n",
    "\n",
    "        states = np.vstack([t[0] for t in batch])\n",
    "        states = torch.from_numpy(states).float().to(DEVICE)\n",
    "\n",
    "        next_states = np.vstack([t[1] for t in batch])\n",
    "        next_states = torch.from_numpy(next_states).float().to(DEVICE)\n",
    "\n",
    "        actions = np.vstack([t[2] for t in batch])\n",
    "        actions = torch.from_numpy(actions).float().to(DEVICE)\n",
    "\n",
    "        rewards = np.vstack([t[3] for t in batch])\n",
    "        rewards = torch.from_numpy(rewards).float().to(DEVICE)\n",
    "\n",
    "        dones = np.vstack([t[4] for t in batch]).astype(np.uint8)\n",
    "        dones = torch.from_numpy(dones).float().to(DEVICE)\n",
    "\n",
    "        # Bellman :\n",
    "        y, state_values = self.calc_bellman(states, next_states, actions, rewards, dones)\n",
    "\n",
    "        loss = self.criterion(y, state_values)\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "        if self.step % UPDATE_EVERY == 0:\n",
    "            self.update_param()\n",
    "\n",
    "    def update_param(self):\n",
    "        for target_param, local_param in zip(self.nnqt.parameters(), self.nnql.parameters()):\n",
    "            target_param.data.copy_(LR * local_param.data + (1.0 - LR) * target_param.data)\n",
    "\n",
    "    def train(self, env, test_env, epochs : int) -> list:\n",
    "        scores = []\n",
    "        train_values = []\n",
    "        test_values = []\n",
    "        eps = EPS_START\n",
    "        self.step = 0\n",
    "        for e in range(epochs):\n",
    "            print(\"epochs nb:\", e, \"epsilon:\", self.epsilon)\n",
    "            episode_score = 0\n",
    "            episode_score2 = 0\n",
    "            test_score = 0\n",
    "            test_score2 = 0\n",
    "            score = 0\n",
    "            state = env.reset()\n",
    "            state = state.reshape(-1, INPUT_DIM)\n",
    "            while True:\n",
    "                action = self.epsilon_greedy_pol(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = next_state.reshape(-1, INPUT_DIM)\n",
    "\n",
    "                self.memory.store((state, next_state, action, reward, done))\n",
    "                self.learn()\n",
    "\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.epsilon = max(self.epsilon * 0.9, 0.1)\n",
    "\n",
    "            episode_score += score\n",
    "            episode_score2 += (env.store['running_capital'][-1] - env.store['running_capital'][0])\n",
    "            scores.append(score)\n",
    "\n",
    "            state = test_env.reset()\n",
    "            done = False\n",
    "            score_te = 0\n",
    "            scores_te = [score_te]\n",
    "            while True:\n",
    "                action = self.epsilon_greedy_pol(state)\n",
    "                next_state, reward, done, _ = test_env.step(action)\n",
    "                next_state = next_state.reshape(-1, STATE_SPACE)\n",
    "                state= next_state\n",
    "                score_te += reward\n",
    "                scores_te.append(score_te)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            test_score += score_te\n",
    "            test_score2 += (test_env.store['running_capital'][-1] - test_env.store['running_capital'][0])\n",
    "\n",
    "            print(f\"Episode: {e}, Train Score: {episode_score:.5f}, Validation Score: {test_score:.5f}\")\n",
    "            print(f\"Episode: {e}, Train Value: ${episode_score2:.5f}, Validation Value: ${test_score2:.5f}\", \"\\n\")\n",
    "            train_values.append(episode_score2)\n",
    "            test_values.append(test_score2)\n",
    "\n",
    "        df = pd.DataFrame(list(zip(env.store['action_store'], env.store['running_capital'], env.store['current_price'])),\n",
    "                                   columns =['Actions', 'Capital', 'Price'])\n",
    "        df.to_csv(\"./out.csv\", index=False)\n",
    "        df_perf = pd.DataFrame(list(zip(train_values, test_values)), columns=['Train', 'Test'])\n",
    "        df_perf.to_csv(\"./perf.csv\")\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGetter:\n",
    "    \"\"\"\n",
    "    The class for getting data for assets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, asset=\"BTC-USD\", start_date=None, end_date=None, freq=\"1d\",\n",
    "                timeframes=[1, 2, 5, 10, 20, 40]):\n",
    "        self.asset = asset\n",
    "        self.sd = start_date\n",
    "        self.ed = end_date\n",
    "        self.freq = freq\n",
    "\n",
    "        self.timeframes = timeframes\n",
    "        self.getData()\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(self.data[:, 1:])\n",
    "\n",
    "\n",
    "    def getData(self):\n",
    "        asset = self.asset\n",
    "        if self.sd is not None and self.ed is not None:\n",
    "            df =  yf.download([asset], start=self.sd, end=self.ed, interval=self.freq)\n",
    "            df_spy = yf.download([\"BTC-USD\"], start=self.sd, end=self.ed, interval=self.freq)\n",
    "        elif self.sd is None and self.ed is not None:\n",
    "            df =  yf.download([asset], end=self.ed, interval=self.freq)\n",
    "            df_spy = yf.download([\"BTC-USD\"], end=self.ed, interval=self.freq)\n",
    "        elif self.sd is not None and self.ed is None:\n",
    "            df =  yf.download([asset], start=self.sd, interval=self.freq)\n",
    "            df_spy = yf.download([\"BTC-USD\"], start=self.sd, interval=self.freq)\n",
    "        else:\n",
    "            df = yf.download([asset], period=\"max\", interval=self.freq)\n",
    "            df_spy = yf.download([\"BTC-USD\"], interval=self.freq)\n",
    "\n",
    "        # Reward - Not included in Observation Space.\n",
    "        df[\"rf\"] = df[\"Adj Close\"].pct_change().shift(-1)\n",
    "\n",
    "        # Returns and Trading Volume Changes\n",
    "        for i in self.timeframes:\n",
    "            df_spy[f\"spy_ret-{i}\"] = df_spy[\"Adj Close\"].pct_change(i)\n",
    "            df_spy[f\"spy_v-{i}\"] = df_spy[\"Volume\"].pct_change(i)\n",
    "\n",
    "            df[f\"r-{i}\"] = df[\"Adj Close\"].pct_change(i)\n",
    "            df[f\"v-{i}\"] = df[\"Volume\"].pct_change(i)\n",
    "\n",
    "        # Volatility\n",
    "        for i in [5, 10, 20, 40]:\n",
    "            df[f'sig-{i}'] = np.log(1 + df[\"r-1\"]).rolling(i).std()\n",
    "\n",
    "        # Moving Average Convergence Divergence (MACD)\n",
    "        df[\"macd_lmw\"] = df[\"r-1\"].ewm(span=26, adjust=False).mean()\n",
    "        df[\"macd_smw\"] = df[\"r-1\"].ewm(span=12, adjust=False).mean()\n",
    "        df[\"macd_bl\"] = df[\"r-1\"].ewm(span=9, adjust=False).mean()\n",
    "        df[\"macd\"] = df[\"macd_smw\"] - df[\"macd_lmw\"]\n",
    "\n",
    "        # Relative Strength Indicator (RSI)\n",
    "        rsi_lb = 5\n",
    "        pos_gain = df[\"r-1\"].where(df[\"r-1\"] > 0, 0).ewm(rsi_lb).mean()\n",
    "        neg_gain = df[\"r-1\"].where(df[\"r-1\"] < 0, 0).ewm(rsi_lb).mean()\n",
    "        rs = np.abs(pos_gain/neg_gain)\n",
    "        df[\"rsi\"] = 100 * rs/(1 + rs)\n",
    "\n",
    "        # Bollinger Bands\n",
    "        bollinger_lback = 10\n",
    "        df[\"bollinger\"] = df[\"r-1\"].ewm(bollinger_lback).mean()\n",
    "        df[\"low_bollinger\"] = df[\"bollinger\"] - 2 * df[\"r-1\"].rolling(bollinger_lback).std()\n",
    "        df[\"high_bollinger\"] = df[\"bollinger\"] + 2 * df[\"r-1\"].rolling(bollinger_lback).std()\n",
    "\n",
    "        # Filtering\n",
    "        for c in df.columns:\n",
    "            df[c].interpolate('linear', limit_direction='both', inplace=True)\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        df.to_csv('./prices.csv')\n",
    "\n",
    "        self.frame = df\n",
    "        self.data = np.array(df.iloc[:, 6:])\n",
    "        return\n",
    "\n",
    "    def scaleData(self):\n",
    "        self.scaled_data = self.scaler.fit_transform(self.data[:, 1:])\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx, col_idx=None):\n",
    "        if col_idx is None:\n",
    "            return self.data[idx]\n",
    "        elif col_idx < len(list(self.data.columns)):\n",
    "            return self.data[idx][col_idx]\n",
    "        else:\n",
    "            raise IndexError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAssetTradingEnvironment:\n",
    "    \"\"\"\n",
    "    Trading Environment for trading a single asset.\n",
    "    The Agent interacts with the environment class through the step() function.\n",
    "    Action Space: {-1: Sell, 0: Do Nothing, 1: Buy}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, asset_data,\n",
    "               initial_money=CAPITAL, trans_cost=COST, store_flag=1, asset_ph=0,\n",
    "               capital_frac=0.2, running_thresh=0.1, cap_thresh=0.3):\n",
    "\n",
    "        self.past_holding = asset_ph\n",
    "        self.capital_frac = capital_frac # Fraction of capital to invest each time.\n",
    "        self.cap_thresh = cap_thresh\n",
    "        self.running_thresh = running_thresh\n",
    "        self.trans_cost = trans_cost\n",
    "\n",
    "        self.asset_data = asset_data\n",
    "        self.terminal_idx = len(self.asset_data) - 1\n",
    "        self.scaler = self.asset_data.scaler\n",
    "\n",
    "        self.initial_cap = initial_money\n",
    "\n",
    "        self.capital = self.initial_cap\n",
    "        self.running_capital = self.capital\n",
    "        self.asset_inv = self.past_holding\n",
    "\n",
    "        self.pointer = 0\n",
    "        self.next_return, self.current_state = 0, None\n",
    "        self.prev_act = 0\n",
    "        self.current_act = 0\n",
    "        self.current_reward = 0\n",
    "        self.current_price = self.asset_data.frame.iloc[self.pointer, :]['Adj Close']\n",
    "        self.done = False\n",
    "\n",
    "        self.store_flag = store_flag\n",
    "        if self.store_flag == 1:\n",
    "            self.store = {\"action_store\": [],\n",
    "                        \"reward_store\": [],\n",
    "                        \"running_capital\": [],\n",
    "                        \"current_price\": [],\n",
    "                        \"port_ret\": []}\n",
    "\n",
    "    def reset(self):\n",
    "        self.capital = self.initial_cap\n",
    "        self.running_capital = self.capital\n",
    "        self.asset_inv = self.past_holding\n",
    "\n",
    "        self.pointer = 0\n",
    "        self.next_return, self.current_state = self.get_state(self.pointer)\n",
    "        self.prev_act = 0\n",
    "        self.current_act = 0\n",
    "        self.current_reward = 0\n",
    "        self.current_price = self.asset_data.frame.iloc[self.pointer, :]['Adj Close']\n",
    "        self.done = False\n",
    "\n",
    "        if self.store_flag == 1:\n",
    "            self.store = {\"action_store\": [],\n",
    "                        \"reward_store\": [],\n",
    "                        \"running_capital\": [],\n",
    "                        \"current_price\": [],\n",
    "                        \"port_ret\": []}\n",
    "\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_act = action\n",
    "        self.current_price = self.asset_data.frame.iloc[self.pointer, :]['Adj Close']\n",
    "        self.current_reward = self.calculate_reward()\n",
    "        self.prev_act = self.current_act\n",
    "        self.pointer += 1\n",
    "        self.next_return, self.current_state = self.get_state(self.pointer)\n",
    "        self.done = self.check_terminal()\n",
    "\n",
    "        if self.done:\n",
    "            reward_offset = 0\n",
    "            ret = (self.store['running_capital'][-1]/self.store['running_capital'][-0]) - 1\n",
    "            if self.pointer < self.terminal_idx:\n",
    "                reward_offset += -1 * max(0.5, 1 - self.pointer/self.terminal_idx)\n",
    "            if self.store_flag:\n",
    "                reward_offset += 10 * ret\n",
    "            self.current_reward += reward_offset\n",
    "\n",
    "        if self.store_flag:\n",
    "            self.store[\"action_store\"].append(self.current_act)\n",
    "            self.store[\"reward_store\"].append(self.current_reward)\n",
    "            self.store[\"running_capital\"].append(self.capital)\n",
    "            self.store[\"current_price\"].append(self.current_price)\n",
    "            info = self.store\n",
    "        else:\n",
    "            info = None\n",
    "\n",
    "        return self.current_state, self.current_reward, self.done, info\n",
    "\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        investment = self.running_capital * self.capital_frac\n",
    "        reward_offset = 0\n",
    "\n",
    "        # Buy Action\n",
    "        if self.current_act == 1:\n",
    "            if self.running_capital > self.initial_cap * self.running_thresh:\n",
    "                self.running_capital -= investment\n",
    "                asset_units = investment/self.current_price\n",
    "                self.asset_inv += asset_units\n",
    "                self.current_price *= (1 - self.trans_cost)\n",
    "\n",
    "        # Sell Action\n",
    "        elif self.current_act == -1:\n",
    "            if self.asset_inv > 0:\n",
    "                self.running_capital += self.asset_inv * self.current_price * (1 - self.trans_cost)\n",
    "                self.asset_inv = 0\n",
    "\n",
    "        # Do Nothing\n",
    "        elif self.current_act == 0:\n",
    "            if self.prev_act == 0:\n",
    "                reward_offset += -0.1\n",
    "            pass\n",
    "\n",
    "        # Reward to give\n",
    "        prev_cap = self.capital\n",
    "        self.capital = self.running_capital + (self.asset_inv) * self.current_price\n",
    "        reward = 100*(self.next_return) * self.current_act - np.abs(self.current_act - self.prev_act) * self.trans_cost\n",
    "        if self.store_flag==1:\n",
    "            self.store['port_ret'].append((self.capital - prev_cap)/prev_cap)\n",
    "\n",
    "        if reward < 0:\n",
    "            reward *= NEG_MUL  # To make the Agent more risk averse towards negative returns.\n",
    "        reward += reward_offset\n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def check_terminal(self):\n",
    "        if self.pointer == self.terminal_idx:\n",
    "            return True\n",
    "        elif self.capital <= self.initial_cap * self.cap_thresh:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def get_state(self, idx):\n",
    "        state = self.asset_data[idx][1:]\n",
    "        state = self.scaler.transform(state.reshape(1, -1))\n",
    "\n",
    "        state = np.concatenate([state, [[self.capital/self.initial_cap,\n",
    "                                        self.running_capital/self.capital,\n",
    "                                        self.asset_inv * self.current_price/self.initial_cap,\n",
    "                                        self.prev_act]]], axis=-1)\n",
    "\n",
    "        next_ret = self.asset_data[idx][0]\n",
    "        return next_ret, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", [\"States\", \"Actions\", \"Rewards\", \"NextStates\", \"Dones\"])\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    Implementation of Agent memory\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity=MEMORY_LEN):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def store(self, t):\n",
    "        self.memory.append(t)\n",
    "\n",
    "    def sample(self, n):\n",
    "        a = random.sample(self.memory, n)\n",
    "        return a\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "epochs nb: 0 epsilon: 1.0\n",
      "Episode: 0, Train Score: -2002.18831, Validation Score: -289.90040\n",
      "Episode: 0, Train Value: $-30614.72360, Validation Value: $-17833.00067 \n",
      "\n",
      "epochs nb: 1 epsilon: 0.9\n",
      "Episode: 1, Train Score: -1776.69489, Validation Score: -314.04708\n",
      "Episode: 1, Train Value: $176674.18744, Validation Value: $-5198.95317 \n",
      "\n",
      "epochs nb: 2 epsilon: 0.81\n",
      "Episode: 2, Train Score: -1299.40595, Validation Score: -111.80590\n",
      "Episode: 2, Train Value: $245065.71533, Validation Value: $-888.98308 \n",
      "\n",
      "epochs nb: 3 epsilon: 0.7290000000000001\n",
      "Episode: 3, Train Score: -1749.00052, Validation Score: -184.25716\n",
      "Episode: 3, Train Value: $47578.33506, Validation Value: $-3490.46129 \n",
      "\n",
      "epochs nb: 4 epsilon: 0.6561000000000001\n",
      "Episode: 4, Train Score: -888.30066, Validation Score: -171.14245\n",
      "Episode: 4, Train Value: $365135.94289, Validation Value: $-6787.79593 \n",
      "\n",
      "epochs nb: 5 epsilon: 0.5904900000000002\n",
      "Episode: 5, Train Score: -862.04491, Validation Score: -188.05989\n",
      "Episode: 5, Train Value: $410321.01936, Validation Value: $-2605.20619 \n",
      "\n",
      "epochs nb: 6 epsilon: 0.5314410000000002\n",
      "Episode: 6, Train Score: -896.03638, Validation Score: -98.93993\n",
      "Episode: 6, Train Value: $948799.39627, Validation Value: $23679.35616 \n",
      "\n",
      "epochs nb: 7 epsilon: 0.47829690000000014\n",
      "Episode: 7, Train Score: -528.34945, Validation Score: -115.50225\n",
      "Episode: 7, Train Value: $339425.97420, Validation Value: $9693.50941 \n",
      "\n",
      "epochs nb: 8 epsilon: 0.43046721000000016\n",
      "Episode: 8, Train Score: -349.45517, Validation Score: -238.89953\n",
      "Episode: 8, Train Value: $1144312.76332, Validation Value: $-9986.17549 \n",
      "\n",
      "epochs nb: 9 epsilon: 0.38742048900000015\n",
      "Episode: 9, Train Score: 87.43637, Validation Score: -192.58546\n",
      "Episode: 9, Train Value: $1988994.64140, Validation Value: $1428.80364 \n",
      "\n",
      "epochs nb: 10 epsilon: 0.34867844010000015\n",
      "Episode: 10, Train Score: 1300.98297, Validation Score: -276.97338\n",
      "Episode: 10, Train Value: $8365140.63441, Validation Value: $-20501.13074 \n",
      "\n",
      "epochs nb: 11 epsilon: 0.31381059609000017\n",
      "Episode: 11, Train Score: 1023.04774, Validation Score: -172.51697\n",
      "Episode: 11, Train Value: $5971764.34927, Validation Value: $-1434.97104 \n",
      "\n",
      "epochs nb: 12 epsilon: 0.28242953648100017\n",
      "Episode: 12, Train Score: 1424.82582, Validation Score: -204.37638\n",
      "Episode: 12, Train Value: $4603315.93914, Validation Value: $13398.19652 \n",
      "\n",
      "epochs nb: 13 epsilon: 0.25418658283290013\n",
      "Episode: 13, Train Score: 1066.26587, Validation Score: -319.85762\n",
      "Episode: 13, Train Value: $4399994.51725, Validation Value: $-4580.64514 \n",
      "\n",
      "epochs nb: 14 epsilon: 0.22876792454961012\n",
      "Episode: 14, Train Score: 274.51107, Validation Score: -180.25342\n",
      "Episode: 14, Train Value: $2321570.74025, Validation Value: $6084.80448 \n",
      "\n",
      "epochs nb: 15 epsilon: 0.2058911320946491\n",
      "Episode: 15, Train Score: 453.30685, Validation Score: -139.76578\n",
      "Episode: 15, Train Value: $5553371.70449, Validation Value: $18165.04973 \n",
      "\n",
      "epochs nb: 16 epsilon: 0.1853020188851842\n",
      "Episode: 16, Train Score: 1542.38923, Validation Score: -118.13437\n",
      "Episode: 16, Train Value: $4829632.53822, Validation Value: $765.05322 \n",
      "\n",
      "epochs nb: 17 epsilon: 0.16677181699666577\n",
      "Episode: 17, Train Score: 1566.43242, Validation Score: -248.07614\n",
      "Episode: 17, Train Value: $1563442.43047, Validation Value: $2287.54234 \n",
      "\n",
      "epochs nb: 18 epsilon: 0.1500946352969992\n",
      "Episode: 18, Train Score: 2092.12037, Validation Score: -216.61228\n",
      "Episode: 18, Train Value: $7953846.90601, Validation Value: $11068.52211 \n",
      "\n",
      "epochs nb: 19 epsilon: 0.13508517176729928\n",
      "Episode: 19, Train Score: 2457.07681, Validation Score: -331.71409\n",
      "Episode: 19, Train Value: $12014339.10364, Validation Value: $15597.57506 \n",
      "\n",
      "epochs nb: 20 epsilon: 0.12157665459056936\n",
      "Episode: 20, Train Score: 2127.31826, Validation Score: -331.25653\n",
      "Episode: 20, Train Value: $11629688.60371, Validation Value: $1691.99061 \n",
      "\n",
      "epochs nb: 21 epsilon: 0.10941898913151243\n",
      "Episode: 21, Train Score: 2109.97152, Validation Score: -221.59811\n",
      "Episode: 21, Train Value: $10884818.05122, Validation Value: $14975.79424 \n",
      "\n",
      "epochs nb: 22 epsilon: 0.1\n",
      "Episode: 22, Train Score: 1211.68691, Validation Score: -274.87012\n",
      "Episode: 22, Train Value: $6819243.17179, Validation Value: $14047.85970 \n",
      "\n",
      "epochs nb: 23 epsilon: 0.1\n",
      "Episode: 23, Train Score: 1962.55693, Validation Score: -323.47009\n",
      "Episode: 23, Train Value: $12030133.45377, Validation Value: $7980.36756 \n",
      "\n",
      "epochs nb: 24 epsilon: 0.1\n",
      "Episode: 24, Train Score: 1654.57738, Validation Score: -217.25659\n",
      "Episode: 24, Train Value: $9483343.27260, Validation Value: $29349.89294 \n",
      "\n",
      "epochs nb: 25 epsilon: 0.1\n",
      "Episode: 25, Train Score: 1402.31481, Validation Score: -380.94707\n",
      "Episode: 25, Train Value: $7328545.08355, Validation Value: $4106.20073 \n",
      "\n",
      "epochs nb: 26 epsilon: 0.1\n",
      "Episode: 26, Train Score: 2925.58004, Validation Score: -326.91890\n",
      "Episode: 26, Train Value: $14065785.03933, Validation Value: $3657.36317 \n",
      "\n",
      "epochs nb: 27 epsilon: 0.1\n",
      "Episode: 27, Train Score: 272.63287, Validation Score: -328.15120\n",
      "Episode: 27, Train Value: $3605761.47529, Validation Value: $3490.17141 \n",
      "\n",
      "epochs nb: 28 epsilon: 0.1\n",
      "Episode: 28, Train Score: 269.03806, Validation Score: -393.80569\n",
      "Episode: 28, Train Value: $2399078.27530, Validation Value: $-4308.35184 \n",
      "\n",
      "epochs nb: 29 epsilon: 0.1\n",
      "Episode: 29, Train Score: 396.52677, Validation Score: -394.48967\n",
      "Episode: 29, Train Value: $3923470.20452, Validation Value: $-9128.32578 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-2002.1883126329099,\n",
       " -1776.6948903998948,\n",
       " -1299.405948515509,\n",
       " -1749.0005199167015,\n",
       " -888.3006643196677,\n",
       " -862.0449132472559,\n",
       " -896.0363783507147,\n",
       " -528.349446372211,\n",
       " -349.45517420990154,\n",
       " 87.43637452375623,\n",
       " 1300.982973107973,\n",
       " 1023.0477413626594,\n",
       " 1424.8258191116072,\n",
       " 1066.2658746891125,\n",
       " 274.51106638286694,\n",
       " 453.30684509233794,\n",
       " 1542.3892333438287,\n",
       " 1566.4324207979937,\n",
       " 2092.1203654244086,\n",
       " 2457.0768052363255,\n",
       " 2127.3182615016885,\n",
       " 2109.9715158644112,\n",
       " 1211.6869109422023,\n",
       " 1962.5569308996432,\n",
       " 1654.5773846450895,\n",
       " 1402.314811555477,\n",
       " 2925.580035397481,\n",
       " 272.6328749715848,\n",
       " 269.0380623522531,\n",
       " 396.5267697825386]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environment and Agent Initiation\n",
    "\n",
    "## Cryptocurrency Tickers\n",
    "asset_code = \"ETH-USD\"\n",
    "\n",
    "## Training and Testing Environments\n",
    "assets = DataGetter(asset_code, start_date=\"2015-01-01\", end_date=\"2021-10-01\")\n",
    "test_assets = DataGetter(asset_code, start_date=\"2021-10-01\", end_date=\"2022-05-01\", freq=\"1d\")\n",
    "env = SingleAssetTradingEnvironment(assets)\n",
    "test_env = SingleAssetTradingEnvironment(test_assets)\n",
    "\n",
    "## Agent\n",
    "memory = ReplayMemory()\n",
    "agent = Agent()\n",
    "\n",
    "# Main training loop\n",
    "N_EPISODES = 30 # No of episodes/epochs\n",
    "\n",
    "agent.train(env, test_env, int(N_EPISODES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
